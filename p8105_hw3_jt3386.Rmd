---
title: "Solutions for Homework 3"
author: "Jiajun Tao"
date: "2022-10-15"
output: github_document
---

```{r setup, include = FALSE,message=FALSE,warning=FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)
library(ggridges)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


### Problem 1

We first read in the data.

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

This dataset has `r nrow(instacart)` rows and `r ncol(instacart)` columns. The variables include `r names(instacart)`. To be specific, `product_id` and `product_name` refer to what products been ordered, `aisle_id` and `aisle` refer to where the products are, and `department_id` and `department` refer to which department the products belong to. The observations document the order id, the user id, how many products in the order, what are the products, where are the products, whether is reordered or not, and so on. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

There are `r instacart %>% distinct(aisle) %>% count` aisles, and fresh vegetables and fresh fruits aisles are the most items ordered from.

Then we make a plot to show the number of items ordered in each aisle.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle,-n)) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_bar(stat = 'identity') +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle",
    y = "Number of Items") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(popular_rank = min_rank(desc(n))) %>%
  filter(popular_rank < 4) %>% 
  arrange(aisle, popular_rank) %>% 
  knitr::kable()
```

Finally we make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(
    order_dow = recode(order_dow,
                       "0" = "Sunday",
                       "1" = "Monday",
                       "2" = "Tuesday",
                       "3" = "Wednesday",
                       "4" = "Thursday",
                       "5" = "Friday",
                       "6" = "Saturday")
  ) %>% 
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hour"
  ) %>% 
  knitr::kable(digit = 2)
```

We could find that Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream except Friday.


### Problem 2

We first load and tidy the data. We add an variable to indicate whether the day is weekday or weekend.

```{r}
accel_df = 
  read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = 'minute',
    values_to = 'activity_counts',
    names_prefix = 'activity_'
  ) %>% 
  mutate(
    minute = as.integer(minute),
    day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
    day_of_week = case_when(
      day == "Saturday" | day == "Sunday" ~ "Weekend",
      day != "Sunday" & day != "Saturday" ~ "Weekday",
      TRUE ~ ""
    )
  )

accel_df
```

The resulting dataset `accel_df` has `r nrow(accel_df)` rows and `r ncol(accel_df)` columns. The variables include `r names(accel_df)`. It shows the activity counts of each minute during 35 days.

We then aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. 
```{r}
accel_df %>% 
  group_by(week,day) %>% 
  summarise(total_activity = sum(activity_counts)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>% 
  knitr::kable()
```

We can see 2 observations show the total activity counts are 1440 with 1 for each minute. That may be a mistake or the patient just did not wear the accelerometers on that day. To be honest, I did not see any trends according to the table because the data fluctuates greatly. Maybe the mean total activity counts is around 300K or 400K, but we need further information to make judgement.

The single-panel plot below shows the 24-hour activity time courses for each day and use color to indicate day of the week.

```{r}
accel_df %>% 
  ggplot(aes(x = minute, y = activity_counts, color = day_of_week)) +
  geom_line(alpha = .5) +
  scale_x_continuous(
    breaks = c(0, 360, 720, 1080, 1440),
    labels = c("0", "6", "12", "18", "24")
  )+
  labs(
    x = "Time from Midnight(h)",
    y = "Activity Counts",
  ) 
```

Based on this graph, generally the activity counts are low during the first 6 hours of a day because of sleeping. The activity counts of weekday is higher in the evening like 19:00 to 22:00, and the activity counts of weekend peek in the noon and afternoon about 17:00.


### Problem 3

We first read the data from `ny_noaa`.

```{r}
data("ny_noaa")
ny_noaa
```

The raw dataset has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The variables include `r names(ny_noaa)`. `id` indicates weather station ID, `date` indicates date of observation, `prcp` indicates precipitation, `snow` indicates snowfall, `snwd` indicates snow depth, `tmax` indicates maximun temperature, and `tmin` indicates minimun temperature. However, `r nrow(drop_na(ny_noaa)) / nrow(ny_noaa) * 100`% of the observations contain missing value.

Then we do some data cleaning.

```{r}
cleaned_noaa_df =   
  ny_noaa %>% 
  separate(date, sep = "-", into = c("year", "month", "day")) %>% 
  mutate(
    month = month.name[as.integer(month)],
    prcp = prcp / 10,
    tmax = as.integer(tmax) / 10,
    tmin = as.integer(tmin) / 10
  )

cleaned_noaa_df %>% 
  count(snow) %>% 
  arrange(-n)
```

For snowfall, the most common value is 0 because there are few days snowing in a year.

Then we make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r}
cleaned_noaa_df %>% 
  filter(month %in% c("January", "July")) %>% 
  group_by(month, id, year) %>% 
  summarise(mean_tmax = mean(tmax, na.rm = TRUE)
  ) %>% 
  ggplot(aes(x = as.integer(year), y = mean_tmax, color = id)) +
  geom_point() + 
  geom_path() +
  facet_grid(. ~ month) +
  labs(
    x = "Year",
    y = "Average Max Temperature (°C)"
  ) +
  theme(legend.position = "none")
```

From the plots, it seems like both average max temperatures in January and July fluctuate with the year. There are several outliers, for example, there was a very low point in January in 1982 and also a very low point in July in 1988.

The two-panel plot below shows (i) `tmax` vs `tmin` for the full dataset; and (ii) the distribution of snowfall values greater than 0 mm and less than 100 mm separately by year.

```{r}
tmax_tmin_plot =   
  cleaned_noaa_df %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  labs(
    x = "Min Temperature (°C)",
    y = "Max Temperature (°C)"
  ) +
  theme(legend.position = "left")

snow_plot =
  cleaned_noaa_df %>% 
  filter(
    snow > 0 & snow < 100
  ) %>% 
  ggplot(aes(x = snow, y = year)) +
  geom_density_ridges() +
  labs(
    x = "Snowfall (mm)",
    y = "Year"
  )

tmax_tmin_plot + snow_plot
```